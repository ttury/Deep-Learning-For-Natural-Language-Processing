{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bag_Of_Words.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMd1/WP+CdmAEgBnB43UOn0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ttury/Deep-Learning-For-Natural-Language-Processing/blob/master/Bag_Of_Words.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EU7NhSJERP5"
      },
      "source": [
        "# **Bag of Words(BoW)**\n",
        "---\n",
        "\n",
        "> 단어의 등장 순서를 고려하지 않고 단어 사전에 포함된 단어의 빈도수를 기록\n",
        "\n",
        "> 정수 인코딩 후 각 단어의 인덱스에 단어의 빈도수 값이 들어가는 리스트 만들기 -> BoW\n",
        "\n",
        "> 단어의 인덱스 할당은 중요하지 않음\n",
        "\n",
        "> 한 문서에서 만든 단어 사전을 기준으로 다른 문서에서 BoW를 만들기도 함\n",
        "\n",
        "<br/>\n",
        "\n",
        "## CountVectorizer\n",
        "---\n",
        "\n",
        "`from sklearn.feature_extraction.text import CountVectorizer`\n",
        "\n",
        "> 빈도 수 기록/코퍼스 입력 : fit_transform()\n",
        "\n",
        "> 정수 인덱싱된 단어 사전 : vocabulary_\n",
        "\n",
        "<br/>\n",
        "\n",
        "## 불용어를 제거한 BoW\n",
        "---\n",
        "\n",
        "> 객체 생성 시 stop_words 매개변수로 입력 가능\n",
        "\n",
        "> 사용자 정의 불용어\n",
        "\n",
        "> CounterVectorizer 지원 불용어\n",
        "\n",
        "> NLTK 지원 불용어\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pLw9oCCo9ol"
      },
      "source": [
        "pip install konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFuiXMvEoL6s"
      },
      "source": [
        "from konlpy.tag import Okt\n",
        "import re\n",
        "\n",
        "okt = Okt()\n",
        "\n",
        "text = \"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\"\n",
        "sent_token = re.sub(\"(\\.)\", \"\", text) # 온점 제거\n",
        "word_tokens = okt.morphs(sent_token) # 단어 토큰화\n",
        "print(word_tokens)\n",
        "print()\n",
        "\n",
        "word2index = dict()\n",
        "bow = list()\n",
        "\n",
        "for word_token in word_tokens:\n",
        "  if word_token not in word2index.keys(): # 단어 사전에 없으면 추가\n",
        "    word2index[word_token] = len(word2index)\n",
        "    bow.insert(word2index[word_token], 1) # 존재하지 않는 인덱스에 값을 넣기 위해 insert 사용\n",
        "  else: # 단어 사전에 있으면 단어 빈도 증가\n",
        "    index = word2index.get(word_token)\n",
        "    bow[index] += 1\n",
        "\n",
        "print(word2index)\n",
        "print(bow)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sR7l129Cvzeb"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "text = 'you know I want your love. because I love you.'\n",
        "corpus = [text] # fit_transform 함수의 매개변수가 리스트\n",
        "print(corpus)\n",
        "vector = CountVectorizer()\n",
        "print(vector.fit_transform(corpus).toarray()) # 코퍼스로부터 각 단어의 빈도 수를 기록\n",
        "print(vector.vocabulary_) # word2index, 단어 사전"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zO5CctFSAHJI"
      },
      "source": [
        "# Custom Stopwords\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "text = \"Family is not an important thing. It's everything\"\n",
        "corpus = [text]\n",
        "stopwords = [\"the\", \"a\", \"an\", \"is\", \"not\"]\n",
        "vector = CountVectorizer(stop_words = stopwords)\n",
        "print(vector.fit_transform(corpus).toarray())\n",
        "print(vector.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Tg9TB4mCoUH"
      },
      "source": [
        "# CountVectorizer Standard Stopwords\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "text = \"Family is not an important thing. It's everything\"\n",
        "corpus = [text]\n",
        "vector = CountVectorizer(stop_words = 'english')\n",
        "print(vector.fit_transform(corpus).toarray())\n",
        "print(vector.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0ZRlDFsDic1"
      },
      "source": [
        "# NLTK Standard Stopwords\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "text = \"Family is not an important thing. It's everything\"\n",
        "corpus = [text]\n",
        "sw = set(stopwords.words('english'))\n",
        "vector = CountVectorizer(stop_words = sw)\n",
        "print(vector.fit_transform(corpus).toarray())\n",
        "print(vector.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}