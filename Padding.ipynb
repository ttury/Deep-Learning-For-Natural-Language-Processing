{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Padding.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNsf6YORIA+hKWoosD4rLa8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ttury/Deep-Learning-For-Natural-Language-Processing/blob/master/Padding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjd8U5PZYNhM"
      },
      "source": [
        "# 패딩(Padding)\n",
        "---\n",
        "> 문장의 길이를 임의로 동일하게 맞춰주는 작업\n",
        "\n",
        "> 정수 인코딩한 단어 집합을 행렬로 만들어 연산하기 쉽게 하기 위해 필요\n",
        "\n",
        "<br/>\n",
        "\n",
        "## Numpy(ndarray)\n",
        "---\n",
        "> 반복문으로 패딩한 뒤 ndarray를 사용해 행렬로 만듬\n",
        "\n",
        "<br/>\n",
        "\n",
        "## Keras(pad_sequences)\n",
        "---\n",
        "> 정수 인코딩이 끝난 단어 집합을 한 번에 패딩과 행렬 변환을 수행함\n",
        "\n",
        "> 기본적으로 패딩시 앞을 0으로 채우기 때문에 뒤를 채우기 위해서는 post 값이 필요\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SpB6pOOQOex",
        "outputId": "764cb9f1-6f22-405a-9dfd-405be85f2638"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hd32702bOr6b",
        "outputId": "7047bae9-2156-45d4-c2e6-89bd8e403ccd"
      },
      "source": [
        "# Numpy\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\"\n",
        "sent_tokens = sent_tokenize(text)\n",
        "stop_words = stopwords.words('english')\n",
        "words_by_sent = list()\n",
        "\n",
        "for sent_token in sent_tokens:\n",
        "  word_tokens = word_tokenize(sent_token)\n",
        "  result = list()\n",
        "\n",
        "  for word_token in word_tokens:\n",
        "    word_token = word_token.lower()\n",
        "    if word_token not in stop_words:\n",
        "      if len(word_token) > 2:\n",
        "        result.append(word_token)\n",
        "\n",
        "  words_by_sent.append(result)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(words_by_sent) # 빈도수를 기준으로 단어 집합 생성\n",
        "encoded = tokenizer.texts_to_sequences(words_by_sent)\n",
        "\n",
        "max_len = max(len(item) for item in encoded) # 한 문장 단위 내 인코딩된 단어 개수 기준\n",
        "print(max_len)\n",
        "\n",
        "for item in encoded:\n",
        "  while len(item) < max_len:\n",
        "    item.append(0) # 남는 뒤 공간을 0으로 채워 패딩\n",
        "\n",
        "padded_np = np.array(encoded)\n",
        "print(padded_np)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7\n",
            "[[ 1  5  0  0  0  0  0]\n",
            " [ 1  8  5  0  0  0  0]\n",
            " [ 1  3  5  0  0  0  0]\n",
            " [ 9  2  0  0  0  0  0]\n",
            " [ 2  4  3  2  0  0  0]\n",
            " [ 3  2  0  0  0  0  0]\n",
            " [ 1  4  6  0  0  0  0]\n",
            " [ 1  4  6  0  0  0  0]\n",
            " [ 1  4  2  0  0  0  0]\n",
            " [ 7  7  3  2 10  1 11]\n",
            " [ 1 12  3 13  0  0  0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FcF7dvzT8HN",
        "outputId": "fe315e2b-83f4-4421-910e-4cc6754e27e2"
      },
      "source": [
        "# keras -> pad_sequences\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\"\n",
        "stop_words = stopwords.words('english')\n",
        "sent_tokens = sent_tokenize(text)\n",
        "words_by_sent = list()\n",
        "\n",
        "for sent_token in sent_tokens:\n",
        "  word_tokens = word_tokenize(sent_token)\n",
        "  result = list()\n",
        "  \n",
        "  for word_token in word_tokens:\n",
        "    word_token = word_token.lower()\n",
        "    if word_token not in stop_words:\n",
        "      if len(word_token) > 2:\n",
        "        result.append(word_token)\n",
        "\n",
        "  words_by_sent.append(result)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(words_by_sent)\n",
        "encoded = tokenizer.texts_to_sequences(words_by_sent)\n",
        "print(encoded)\n",
        "print()\n",
        "\n",
        "padded = pad_sequences(encoded) # pad_sequences를 이용한 패딩\n",
        "print(padded)\n",
        "print()\n",
        "\n",
        "padded_post = pad_sequences(encoded, padding = \"post\") # 뒤에 0 채우기\n",
        "print(padded_post)\n",
        "print()\n",
        "\n",
        "padded_maxlen = pad_sequences(encoded, padding = \"post\", maxlen = 5) # 길이 제한(넘어가면 데이터 손실)\n",
        "print(padded_maxlen)\n",
        "print()\n",
        "\n",
        "last_value = len(tokenizer.word_index) + 1 # 단어 집합의 인코딩에 사용되지 않은 정수 구하기\n",
        "print(last_value)\n",
        "print()\n",
        "\n",
        "padded = pad_sequences(encoded, padding = 'post', value = last_value)\n",
        "print(padded)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n",
            "\n",
            "[[ 0  0  0  0  0  1  5]\n",
            " [ 0  0  0  0  1  8  5]\n",
            " [ 0  0  0  0  1  3  5]\n",
            " [ 0  0  0  0  0  9  2]\n",
            " [ 0  0  0  2  4  3  2]\n",
            " [ 0  0  0  0  0  3  2]\n",
            " [ 0  0  0  0  1  4  6]\n",
            " [ 0  0  0  0  1  4  6]\n",
            " [ 0  0  0  0  1  4  2]\n",
            " [ 7  7  3  2 10  1 11]\n",
            " [ 0  0  0  1 12  3 13]]\n",
            "\n",
            "[[ 1  5  0  0  0  0  0]\n",
            " [ 1  8  5  0  0  0  0]\n",
            " [ 1  3  5  0  0  0  0]\n",
            " [ 9  2  0  0  0  0  0]\n",
            " [ 2  4  3  2  0  0  0]\n",
            " [ 3  2  0  0  0  0  0]\n",
            " [ 1  4  6  0  0  0  0]\n",
            " [ 1  4  6  0  0  0  0]\n",
            " [ 1  4  2  0  0  0  0]\n",
            " [ 7  7  3  2 10  1 11]\n",
            " [ 1 12  3 13  0  0  0]]\n",
            "\n",
            "[[ 1  5  0  0  0]\n",
            " [ 1  8  5  0  0]\n",
            " [ 1  3  5  0  0]\n",
            " [ 9  2  0  0  0]\n",
            " [ 2  4  3  2  0]\n",
            " [ 3  2  0  0  0]\n",
            " [ 1  4  6  0  0]\n",
            " [ 1  4  6  0  0]\n",
            " [ 1  4  2  0  0]\n",
            " [ 3  2 10  1 11]\n",
            " [ 1 12  3 13  0]]\n",
            "\n",
            "14\n",
            "\n",
            "[[ 1  5 14 14 14 14 14]\n",
            " [ 1  8  5 14 14 14 14]\n",
            " [ 1  3  5 14 14 14 14]\n",
            " [ 9  2 14 14 14 14 14]\n",
            " [ 2  4  3  2 14 14 14]\n",
            " [ 3  2 14 14 14 14 14]\n",
            " [ 1  4  6 14 14 14 14]\n",
            " [ 1  4  6 14 14 14 14]\n",
            " [ 1  4  2 14 14 14 14]\n",
            " [ 7  7  3  2 10  1 11]\n",
            " [ 1 12  3 13 14 14 14]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}