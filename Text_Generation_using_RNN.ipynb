{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Generation_using_RNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNrBdAKHRmSa4HeleqruEMt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ttury/Deep-Learning-For-Natural-Language-Processing/blob/master/Text_Generation_using_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRSvcTJ3cSIi"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "text=\"\"\"경마장에 있는 말이 뛰고 있다\n",
        "그의 말이 법이다\n",
        "가는 말이 고와야 오는 말이 곱다\"\"\"\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "print(tokenizer.word_index)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "# 케라스 토크나이저의 정수 인코딩은 인덱스가 1부터 시작하지만,\n",
        "# 케라스 원-핫 인코딩은 인덱스가 0부터 시작하기 때문에\n",
        "# 인덱스를 1부터 시작하기 위해 단어 집합의 크기보다 1 크게 지정\n",
        "\n",
        "print('단어 집합의 크기 : %d' % vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hdg7erhbdyeG"
      },
      "source": [
        "sequences = list()\n",
        "for line in text.split('\\n'):\n",
        "  print(line)\n",
        "  encoded = tokenizer.texts_to_sequences([line])[0]\n",
        "  print(encoded)\n",
        "  # 정수 인코딩, 2차원 리스트지만 한 문장밖에 없으므로 첫 원소가 전체 데이터와 같음 \n",
        "  for i in range(1, len(encoded)):\n",
        "    sequence = encoded[:i+1] # 한 문장의 완성 과정을 분리 ex) 그의 / 그의 말이 / 그의 말이 법이다\n",
        "    sequences.append(sequence)\n",
        "\n",
        "print()\n",
        "print('학습에 사용할 샘플의 개수: {}'.format(len(sequences)))\n",
        "print(sequences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGBU49J8fnZf"
      },
      "source": [
        "max_len = max(len(sequence) for sequence in sequences) # 가장 긴 샘플을 기준으로 샘플 길이를 패딩 -> 6으로 패딩\n",
        "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre') # pre 값을 주면 앞에서부터 0으로 채움\n",
        "\n",
        "print(sequences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT2q_g51gO-D"
      },
      "source": [
        "# 각 샘플의 마지막 단어를 레이블로 분리해서 정답으로 사용\n",
        "\n",
        "sequences = np.array(sequences)\n",
        "x = sequences[:, :-1]\n",
        "y = sequences[:, -1]\n",
        "\n",
        "print(x)\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Snmpvcm_g6zu"
      },
      "source": [
        "# one-hot-encoding\n",
        "\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xd7UOhCEhN7p"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, SimpleRNN\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length=max_len-1)) # (vocab_size x 10) 크기의 임베딩 행렬\n",
        "# x와 y를 분리해서 x의 길이가 1 줄었다\n",
        "model.add(SimpleRNN(32)) # hidden_size = 32\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(x, y, epochs=200, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igN-SCAAjgYi"
      },
      "source": [
        "def generate_sent(model, tokenizer, current_word, n):\n",
        "  sentence = current_word\n",
        "\n",
        "  for _ in range(n):\n",
        "    encoded = tokenizer.texts_to_sequences([current_word])[0]\n",
        "    encoded = pad_sequences([encoded], maxlen=5, padding='pre')\n",
        "    result = model.predict_classes(encoded, verbose=0)\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "      if index == result:\n",
        "        current_word = current_word + ' ' + word\n",
        "        sentence = sentence + ' ' + word\n",
        "        break\n",
        "\n",
        "  return sentence"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "ssvsRJ40lYq_",
        "outputId": "f7e95878-6c85-4467-e866-83ac99bc4d2c"
      },
      "source": [
        "print(generate_sent(model, tokenizer, '경마장에', 4))\n",
        "print(generate_sent(model, tokenizer, '그의', 2))\n",
        "print(generate_sent(model, tokenizer, '가는', 5))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ebdecc0c241a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_sent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'경마장에'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_sent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'그의'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_sent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'가는'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'generate_sent' is not defined"
          ]
        }
      ]
    }
  ]
}