{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "One_Hot_Encoding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOupQ7eya3vfuKu2Ut/hL68",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ttury/Deep-Learning-For-Natural-Language-Processing/blob/master/One_Hot_Encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s35_-m-xThcX"
      },
      "source": [
        "pip install konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ili1oB-CR0lL"
      },
      "source": [
        "# konlpy의 Okt 형태소 분석기로 토큰화\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "okt = Okt()\n",
        "word_tokens = okt.morphs(\"나는 자연어 처리를 배운다\")\n",
        "print(word_tokens)\n",
        "\n",
        "vocab = dict()\n",
        "for word_token in word_tokens:\n",
        "  if word_token not in vocab.keys():\n",
        "    vocab[word_token] = len(vocab)\n",
        "\n",
        "def one_hot_encoding(word, vocab):\n",
        "  one_hot_vector = [0] * (len(vocab))\n",
        "  index = vocab[word]\n",
        "  one_hot_vector[index] = 1\n",
        "  return one_hot_vector\n",
        "\n",
        "print(one_hot_encoding(\"자연어\", vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bm06BjEDbZoz"
      },
      "source": [
        "# One-Hot Encoding\n",
        "---\n",
        "> 정수 인덱스를 부여한 단어 집합에서 단어를 연산하기 쉬운 벡터로 변환하는 표현 방법\n",
        "\n",
        "> 단어 집합의 크기를 벡터의 차원으로 하고, 표현 단어의 인덱스에는 1을, 나머지 인덱스에는 0을 부여한다\n",
        "\n",
        "<br/>\n",
        "\n",
        "## 단어 집합을 매개변수로 하는 함수 이용\n",
        "---\n",
        "\n",
        "> 1. 단어 집합의 크기를 벡터의 차원으로 할당한다\n",
        "2. 표현 단어(정수 인코딩됨)의 인덱스에 1을 부여한다\n",
        "3. One-Hot Vector를 반환한다.\n",
        "\n",
        "<br/>\n",
        "\n",
        "## Keras -> to_categorical\n",
        "---\n",
        "\n",
        "> 정수 인코딩된 단어 집합을 입력받아 각 단어별 One-Hot Vector를 포함하는 행렬을 생성한다.\n",
        "\n",
        "<br/>\n",
        "\n",
        "## 한계\n",
        "---\n",
        "\n",
        "> 단어의 개수가 늘어날수록, 벡터의 차원이 계속 늘어남\n",
        "\n",
        "> 벡터 중 하나의 값만 유효해 메모리 활용이 비효율적임\n",
        "\n",
        "> 단어 간 유사도를 표현하지 못함\n",
        "\n",
        "<br/>\n",
        "\n",
        "## 해결책\n",
        "\n",
        "> 카운트 기반 : LSA, HAL\n",
        "\n",
        "> 예측 기반 : NNLM, RNNLM, Word2Vec, FastText\n",
        "\n",
        "<br/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7h4jAOOiX9eK",
        "outputId": "b2749609-17ae-4fe9-8552-f2d637d26c63"
      },
      "source": [
        "# keras -> to_categorical\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "text = \"나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\"\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "print(tokenizer.word_index)\n",
        "\n",
        "sub_text = \"점심 먹으러 갈래 메뉴는 햄버거 최고야\"\n",
        "encoded = tokenizer.texts_to_sequences([sub_text])[0]\n",
        "print(encoded)\n",
        "print()\n",
        "\n",
        "one_hot = to_categorical(encoded) # one_hot_vector로 이루어진 행렬\n",
        "print(one_hot)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'갈래': 1, '점심': 2, '햄버거': 3, '나랑': 4, '먹으러': 5, '메뉴는': 6, '최고야': 7}\n",
            "[2, 5, 1, 6, 3, 7]\n",
            "\n",
            "[[0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}