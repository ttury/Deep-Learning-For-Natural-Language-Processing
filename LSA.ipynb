{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMY33anAzsme+y8YYPc52jh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ttury/Deep-Learning-For-Natural-Language-Processing/blob/master/LSA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFWBQyVZVB1s"
      },
      "source": [
        "# 실습에 사용한 DTM\n",
        "\n",
        "|-|과일이|길고|노란|먹고|바나나|사과|싶은|저는|좋아요|\n",
        "|---|---|---|---|---|---|---|---|---|---|\n",
        "|문서 1|0|0|0|1|0|1|1|0|0|\n",
        "|문서 2|0|0|0|1|1|0|1|0|0|\n",
        "|문서 3|0|1|1|0|2|0|0|0|0|\n",
        "|문서 4|1|0|0|0|0|0|0|1|1|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6x_acceNSoEU",
        "outputId": "4a4a362c-1f44-4978-df9e-e3d0b68dc2f6"
      },
      "source": [
        "import numpy as np\n",
        "A = np.array([[0, 0, 0, 1, 0, 1, 1, 0, 0], [0, 0, 0, 1, 1, 0, 1, 0, 0], [0, 1, 1, 0, 2, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 1, 1]])\n",
        "m, n = np.shape(A)\n",
        "print(m, n)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3WHl-KLWa1A",
        "outputId": "30186029-4c86-46ad-f4ad-e9e8a5c99732"
      },
      "source": [
        "# U = m x m 직교행렬, V = n x n 직교행렬, S = m x n 직사각 대각행렬\n",
        "U, s, VT = np.linalg.svd(A, full_matrices = True) # full SVD\n",
        "\n",
        "# U\n",
        "print(U.round(2)) # 소수점 2번째 자리까지\n",
        "print(np.shape(U))\n",
        "print()\n",
        "\n",
        "# S\n",
        "print(s.round(2)) # 대각 행렬 전체가 아닌 특이값(대각선 성분, 내림차순)만 존재, \n",
        "print(np.shape(s))\n",
        "print()\n",
        "\n",
        "S = np.zeros((m, n)) # 대각 행렬 형태 생성\n",
        "S[:4, :4] = np.diag(s)\n",
        "# ndarray는 튜플로 슬라이싱 가능, np.diag로 대각행렬 만듬(매개변수에 따라 달라짐 벡터 -> 행렬, 행렬 -> 벡터)\n",
        "print(S.round(2))\n",
        "print(S.shape)\n",
        "print()\n",
        "\n",
        "# V의 전치 행렬\n",
        "print(VT.round(2))\n",
        "print(VT.shape)\n",
        "print()\n",
        "\n",
        "print(np.allclose(A, np.dot(np.dot(U, S), VT).round(2)))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.24  0.75  0.   -0.62]\n",
            " [-0.51  0.44 -0.    0.74]\n",
            " [-0.83 -0.49 -0.   -0.27]\n",
            " [-0.   -0.    1.    0.  ]]\n",
            "(4, 4)\n",
            "\n",
            "[2.69 2.05 1.73 0.77]\n",
            "(4,)\n",
            "\n",
            "[[2.69 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   2.05 0.   0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   1.73 0.   0.   0.   0.   0.   0.  ]\n",
            " [0.   0.   0.   0.77 0.   0.   0.   0.   0.  ]]\n",
            "(4, 9)\n",
            "\n",
            "[[-0.   -0.31 -0.31 -0.28 -0.8  -0.09 -0.28 -0.   -0.  ]\n",
            " [ 0.   -0.24 -0.24  0.58 -0.26  0.37  0.58 -0.   -0.  ]\n",
            " [ 0.58 -0.    0.    0.   -0.    0.   -0.    0.58  0.58]\n",
            " [ 0.   -0.35 -0.35  0.16  0.25 -0.8   0.16 -0.   -0.  ]\n",
            " [-0.   -0.78 -0.01 -0.2   0.4   0.4  -0.2   0.    0.  ]\n",
            " [-0.29  0.31 -0.78 -0.24  0.23  0.23  0.01  0.14  0.14]\n",
            " [-0.29 -0.1   0.26 -0.59 -0.08 -0.08  0.66  0.14  0.14]\n",
            " [-0.5  -0.06  0.15  0.24 -0.05 -0.05 -0.19  0.75 -0.25]\n",
            " [-0.5  -0.06  0.15  0.24 -0.05 -0.05 -0.19 -0.25  0.75]]\n",
            "(9, 9)\n",
            "\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8-yF79ZD-Eu",
        "outputId": "729134a0-1802-4fcd-b246-f83c9ab139e3"
      },
      "source": [
        "# Truncated SVD (절단된 SVD)\n",
        "\n",
        "t = 2 # 토픽 개수를 정하는 하이퍼파라미터\n",
        "\n",
        "S = S[:t, :t] # S의 경우 t x t로 자름\n",
        "print(S.round(2))\n",
        "print()\n",
        "\n",
        "U = U[:, :t] # 문서의 개수 x 토픽의 개수 -> 문서의 잠재 의미 표현\n",
        "print(U.round(2))\n",
        "print()\n",
        "\n",
        "VT = VT[:t, :] # 토픽의 개수 x 단어의 개수 -> 단어의 잠재 의미 표현\n",
        "print(VT.round(2))\n",
        "print()\n",
        "\n",
        "A_prime = np.dot(np.dot(U, S), VT)\n",
        "print(A)\n",
        "print()\n",
        "print(A_prime.round(2))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2.69 0.  ]\n",
            " [0.   2.05]]\n",
            "\n",
            "[[-0.24  0.75]\n",
            " [-0.51  0.44]\n",
            " [-0.83 -0.49]\n",
            " [-0.   -0.  ]]\n",
            "\n",
            "[[-0.   -0.31 -0.31 -0.28 -0.8  -0.09 -0.28 -0.   -0.  ]\n",
            " [ 0.   -0.24 -0.24  0.58 -0.26  0.37  0.58 -0.   -0.  ]]\n",
            "\n",
            "[[0 0 0 1 0 1 1 0 0]\n",
            " [0 0 0 1 1 0 1 0 0]\n",
            " [0 1 1 0 2 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 1 1]]\n",
            "\n",
            "[[ 0.   -0.17 -0.17  1.08  0.12  0.62  1.08 -0.   -0.  ]\n",
            " [ 0.    0.2   0.2   0.91  0.86  0.45  0.91  0.    0.  ]\n",
            " [ 0.    0.93  0.93  0.03  2.05 -0.17  0.03  0.    0.  ]\n",
            " [ 0.    0.    0.    0.    0.   -0.    0.    0.    0.  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dRWy-JoF5D_",
        "outputId": "e58beb95-b546-442e-c495-163637f0fa49"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_20newsgroups # 20개의 다른 주제를 가진 뉴스그룹 데이터\n",
        "\n",
        "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
        "documents = dataset.data\n",
        "len(documents)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11314"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-yU9uGwHdA0",
        "outputId": "2b290237-24b4-4180-f017-5dc7134c337a"
      },
      "source": [
        "print(documents[1])\n",
        "print()\n",
        "print(dataset.target_names)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Yeah, do you expect people to read the FAQ, etc. and actually accept hard\n",
            "atheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\n",
            "of steam!\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Jim,\n",
            "\n",
            "Sorry I can't pity you, Jim.  And I'm sorry that you have these feelings of\n",
            "denial about the faith you need to get by.  Oh well, just pretend that it will\n",
            "all end happily ever after anyway.  Maybe if you start a new newsgroup,\n",
            "alt.atheist.hard, you won't be bummin' so much?\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Bye-Bye, Big Jim.  Don't forget your Flintstone's Chewables!  :) \n",
            "--\n",
            "Bake Timmons, III\n",
            "\n",
            "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIrNMsYDH6hK"
      },
      "source": [
        "news_df = pd.DataFrame({'document':documents})\n",
        "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \") # 알파벳 외 삭제\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w) > 3]))\n",
        "# 길이 3 이하인 단어 삭제, (구분자).join(리스트) -> 리스트의 원소를 구분자로 구분해 합침\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_13jjL4MjMO",
        "outputId": "24ab7cec-495f-4567-d035-d9f408179fca"
      },
      "source": [
        "print(news_df['clean_doc'][1])"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "yeah expect people read actually accept hard atheism need little leap faith jimmy your logic runs steam sorry pity sorry that have these feelings denial about faith need well just pretend that will happily ever after anyway maybe start newsgroup atheist hard bummin much forget your flintstone chewables bake timmons\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaMqPVz5M83Q",
        "outputId": "a791ae4d-a928-4569-c3dd-d096c33a5e83"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98bH3hiRM02J",
        "outputId": "f5ed68ef-5be4-417a-a796-da64e9e20c0d"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "tokenized_doc = news_df['clean_doc'].apply(lambda x : x.split())\n",
        "tokenized_doc = tokenized_doc.apply(lambda x : [item for item in x if x not in stop_words])\n",
        "\n",
        "print(tokenized_doc[1])"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['yeah', 'expect', 'people', 'read', 'actually', 'accept', 'hard', 'atheism', 'need', 'little', 'leap', 'faith', 'jimmy', 'your', 'logic', 'runs', 'steam', 'sorry', 'pity', 'sorry', 'that', 'have', 'these', 'feelings', 'denial', 'about', 'faith', 'need', 'well', 'just', 'pretend', 'that', 'will', 'happily', 'ever', 'after', 'anyway', 'maybe', 'start', 'newsgroup', 'atheist', 'hard', 'bummin', 'much', 'forget', 'your', 'flintstone', 'chewables', 'bake', 'timmons']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkxwu-UzN2j3",
        "outputId": "51a90a6a-448a-42c6-ee27-8d3f353606a0"
      },
      "source": [
        "# sklearn의 TfidfVectorizer는 토큰화가 되어 있지 않은 텍스트를 사용하기 때문에 역토큰화 수행\n",
        "\n",
        "detokenized_doc = list()\n",
        "for i in range(len(news_df)):\n",
        "  t = ' '.join(tokenized_doc[i])\n",
        "  detokenized_doc.append(t)\n",
        "\n",
        "print(detokenized_doc[1])"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "yeah expect people read actually accept hard atheism need little leap faith jimmy your logic runs steam sorry pity sorry that have these feelings denial about faith need well just pretend that will happily ever after anyway maybe start newsgroup atheist hard bummin much forget your flintstone chewables bake timmons\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3JJpnmYOmVh",
        "outputId": "5c8b8160-63a1-45c4-df22-41b967f1b9f9"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "MAX_FEATURES = 1000 # TF-IDF 행렬을 만들 때 사용할 최대 단어 개수\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english',\n",
        "                             max_features = MAX_FEATURES,\n",
        "                             max_df = 0.5,\n",
        "                             smooth_idf = True)\n",
        "\n",
        "TF_IDF = vectorizer.fit_transform(news_df['clean_doc'])\n",
        "print(TF_IDF.shape)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11314, 1000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwtjhsSsPhK-",
        "outputId": "1065008e-a9b5-4ccd-dd12-679a91d1eb6d"
      },
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "svd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122)\n",
        "svd_model.fit(TF_IDF)\n",
        "print(svd_model.components_.shape) # 절단된 SVD 중 VT(토픽의 수 x 단어의 수)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20, 1000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6sihuVARjYF",
        "outputId": "e5aac1e2-ea55-4abe-ba41-4dfcdae01e94"
      },
      "source": [
        "terms = vectorizer.get_feature_names()\n",
        "\n",
        "def get_topics(components, feature_names, n=5):\n",
        "  for index, topic in enumerate(components):\n",
        "    print(\"Topic %d:\" %(index + 1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1 :-1]])\n",
        "    '''\n",
        "    print에서 %를 사용하면 변수 출력 가능\n",
        "    argsort 함수 : numpy에서 사용가능, 오름차순 정렬했을 때의 인덱스(value 아님)의 순서 출력\n",
        "    ex) 1 6 4 2 3 -> 0 3 4 2 1\n",
        "    [:-n - 1 :-1]  -> argsort가 오름차순이므로 큰 값부터 n개의 값을 출력하기 위해 사용\n",
        "    '''\n",
        "\n",
        "get_topics(svd_model.components_, terms)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic 1: [('just', 0.20887), ('like', 0.20469), ('know', 0.19349), ('people', 0.18318), ('think', 0.1697)]\n",
            "Topic 2: [('thanks', 0.32763), ('windows', 0.28786), ('card', 0.18019), ('drive', 0.16864), ('mail', 0.15261)]\n",
            "Topic 3: [('game', 0.34011), ('team', 0.30311), ('year', 0.26894), ('games', 0.23784), ('drive', 0.17472)]\n",
            "Topic 4: [('drive', 0.46159), ('scsi', 0.17188), ('disk', 0.14451), ('hard', 0.13805), ('problem', 0.12763)]\n",
            "Topic 5: [('drive', 0.39993), ('know', 0.28768), ('thanks', 0.24917), ('does', 0.24678), ('just', 0.17387)]\n",
            "Topic 6: [('just', 0.55559), ('like', 0.23559), ('windows', 0.23078), ('know', 0.15795), ('does', 0.11156)]\n",
            "Topic 7: [('just', 0.43264), ('like', 0.22858), ('mail', 0.15052), ('bike', 0.11698), ('thanks', 0.10025)]\n",
            "Topic 8: [('does', 0.39692), ('know', 0.25192), ('chip', 0.22492), ('like', 0.17824), ('card', 0.15695)]\n",
            "Topic 9: [('like', 0.42065), ('card', 0.32249), ('sale', 0.20267), ('video', 0.1571), ('offer', 0.14119)]\n",
            "Topic 10: [('like', 0.61166), ('drive', 0.23998), ('file', 0.13257), ('files', 0.09233), ('sounds', 0.08533)]\n",
            "Topic 11: [('people', 0.44189), ('like', 0.25647), ('thanks', 0.19072), ('card', 0.18615), ('government', 0.18341)]\n",
            "Topic 12: [('think', 0.66867), ('good', 0.25984), ('thanks', 0.11249), ('need', 0.10479), ('chip', 0.09178)]\n",
            "Topic 13: [('think', 0.36273), ('does', 0.22264), ('just', 0.21194), ('mail', 0.12875), ('like', 0.12095)]\n",
            "Topic 14: [('know', 0.36546), ('good', 0.30975), ('people', 0.27825), ('windows', 0.2729), ('file', 0.20088)]\n",
            "Topic 15: [('space', 0.4519), ('know', 0.31141), ('think', 0.18571), ('nasa', 0.1711), ('card', 0.11724)]\n",
            "Topic 16: [('does', 0.42324), ('israel', 0.31948), ('think', 0.26996), ('right', 0.1863), ('israeli', 0.17007)]\n",
            "Topic 17: [('good', 0.41859), ('space', 0.27501), ('card', 0.1855), ('does', 0.16985), ('thanks', 0.16633)]\n",
            "Topic 18: [('people', 0.5173), ('does', 0.25965), ('window', 0.21657), ('problem', 0.19057), ('space', 0.12958)]\n",
            "Topic 19: [('right', 0.36578), ('bike', 0.31154), ('time', 0.19438), ('windows', 0.18916), ('space', 0.1777)]\n",
            "Topic 20: [('file', 0.53334), ('problem', 0.20198), ('files', 0.19583), ('need', 0.18333), ('time', 0.15802)]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}