{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNYHGL7/mIOyqEtsnnsbntB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ttury/Deep-Learning-For-Natural-Language-Processing/blob/master/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MThMngUqGQ5U"
      },
      "source": [
        "# **RNN**\n",
        "> ### Recursive Neural Network\n",
        "---\n",
        "<br/>\n",
        "\n",
        "## 원리\n",
        "\n",
        "> 하나의 은닉층이 하나의 시점 -> 여러 개의 시점을 순환\n",
        "\n",
        "> 기존 모델이 입력층 -> 은닉층 -> 출력층의 수직 구조였다면 RNN은 같은 은닉층 간에 수평으로 이동하는 구조\n",
        "\n",
        "> 시점(timesteps)이라는 차원이 늘어났으므로 입력으로 3D 텐서를 받음\n",
        "\n",
        "<br/>\n",
        "\n",
        "## 구현\n",
        "\n",
        "> 입력으로 받는 3D 텐서는 (**batch_size** X **timesteps** or **input_length** X **input_dim**) 크기\n",
        "\n",
        "> hidden_size(output_dim)는 은닉층의 출력값의 크기를 결정\n",
        "\n",
        "> 마지막 은닉층의 상태만 출력하는 경우 (batch_size x hidden_size) 크기로 출력\n",
        "\n",
        "> 전체 은닉층의 상태를 출력하는 경우 (batch_size x timesteps x output_dim) 크기로 출력\n",
        "\n",
        "> 은닉 상태 h_t = tanh(w_x * x_t + w_h * h_t-1 + b)\n",
        "\n",
        "> x_t는 (input_size x 1) 크기\n",
        "\n",
        "> h_t는 (hidden_size x 1) 크기\n",
        "\n",
        "> w_x는 (hidden_size x input_size) 크기\n",
        "\n",
        "> w_h는 (hidden_size x hidden_size) 크기\n",
        "\n",
        "> b는 (hidden_size x 1) 크기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32ZY3NnK2TW7",
        "outputId": "9a618bfe-e0fc-404f-cacd-2b2b1918b585"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import SimpleRNN\n",
        "\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(3, input_shape=(2, 10)))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn (SimpleRNN)       (None, 3)                 42        \n",
            "=================================================================\n",
            "Total params: 42\n",
            "Trainable params: 42\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wj1fzDt6d8k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b93ab0ae-b14f-457f-ddf1-cbb68c6b4c86"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "timesteps = 10 # NLP에서는 문장의 길이\n",
        "input_dim = 4 # NLP에서는 단어 사전의 크기\n",
        "hidden_size = 8 # 은닉층의 출력값 1개가 (8 x 1)의 1D 텐서가 됨\n",
        "\n",
        "inputs = np.random.random((timesteps, input_dim))\n",
        "hidden_state_t = np.zeros((hidden_size,))\n",
        "\n",
        "print(hidden_state_t)\n",
        "print(inputs)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[[0.33756266 0.22161874 0.41178249 0.90963119]\n",
            " [0.76197198 0.89811664 0.70320674 0.80304199]\n",
            " [0.3097855  0.04566501 0.7419276  0.56685688]\n",
            " [0.59309001 0.29384725 0.2686694  0.54316667]\n",
            " [0.96906044 0.30837134 0.47638066 0.4594827 ]\n",
            " [0.78192377 0.6352608  0.73435426 0.5669324 ]\n",
            " [0.99497301 0.66537947 0.75617897 0.01558322]\n",
            " [0.64125047 0.79644924 0.83583904 0.78995964]\n",
            " [0.957643   0.06593918 0.08117109 0.99145702]\n",
            " [0.97540096 0.40538387 0.40686918 0.62071768]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpLzGKcb94nk",
        "outputId": "190256ce-9a3b-4548-c144-ec0b56232ab4"
      },
      "source": [
        "wx = np.random.random((hidden_size, input_dim)) # wx * input_t = (8 x 4) * (4 x 1)  = (8 x 1)\n",
        "wh = np.random.random((hidden_size, hidden_size)) # wh * hidden_state_t = (8 x 8) * (8 x 1) = (8 x 1)\n",
        "b = np.random.random((hidden_size,))\n",
        "# hidden_state_t + 1 = wx * input_t + wh * hidden_state_t + b = (8 x 1) + (8 x 1) + (8 x 1) = (8 x 1)\n",
        "\n",
        "print(wx.shape)\n",
        "print(wh.shape)\n",
        "print(b.shape)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8, 4)\n",
            "(8, 8)\n",
            "(8,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UNXnJx1_rF4",
        "outputId": "fa1cf048-a909-4927-9916-6c79ddbfb4da"
      },
      "source": [
        "total_hidden_states = []\n",
        "\n",
        "for input_t in inputs:\n",
        "  output_t = np.tanh(np.dot(wx, input_t) + np.dot(wh, hidden_state_t) + b)\n",
        "  total_hidden_states.append(list(output_t))\n",
        "  print(np.shape(total_hidden_states))\n",
        "  hidden_state_t = output_t\n",
        "\n",
        "total_hidden_states = np.stack(total_hidden_states, axis=0)\n",
        "\n",
        "print(total_hidden_states)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 8)\n",
            "(2, 8)\n",
            "(3, 8)\n",
            "(4, 8)\n",
            "(5, 8)\n",
            "(6, 8)\n",
            "(7, 8)\n",
            "(8, 8)\n",
            "(9, 8)\n",
            "(10, 8)\n",
            "[[0.99969659 0.99995897 0.99999848 0.99955777 0.99997816 0.99974038\n",
            "  0.99974013 0.99994561]\n",
            " [0.99994586 0.99998817 0.99999963 0.99977261 0.9999893  0.99993549\n",
            "  0.99988576 0.99998415]\n",
            " [0.99953565 0.99997109 0.99999829 0.99955297 0.99995267 0.99977215\n",
            "  0.9996605  0.99996038]\n",
            " [0.99967491 0.9999552  0.99999814 0.99951773 0.99996487 0.9997545\n",
            "  0.99965765 0.99994661]\n",
            " [0.99979483 0.99998014 0.99999922 0.99969018 0.99996481 0.99986622\n",
            "  0.99974544 0.99997705]\n",
            " [0.9998966  0.9999867  0.99999946 0.99973585 0.99997872 0.99991715\n",
            "  0.99983036 0.99998276]\n",
            " [0.99986859 0.9999875  0.99999928 0.99970347 0.99995062 0.99992771\n",
            "  0.99974492 0.9999851 ]\n",
            " [0.99993284 0.99998876 0.9999996  0.999765   0.99998738 0.99993193\n",
            "  0.99987802 0.99998429]\n",
            " [0.99974294 0.99996477 0.99999919 0.99967675 0.99998103 0.99975375\n",
            "  0.9997706  0.99996104]\n",
            " [0.99984503 0.99997993 0.99999935 0.99971042 0.99997599 0.99987292\n",
            "  0.99978982 0.9999767 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}